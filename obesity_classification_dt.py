# -*- coding: utf-8 -*-
"""Obesity Classification-DT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_W__F2oDUGR6usX5LFAqxvhIF2D8n2Kz

##Introduction of the Dataset
###The Obesity Classification Dataset is a dataset commonly used in machine learning for classification tasks, particularly in the domain of healthcare and medical research. This dataset typically contains various features related to individuals' lifestyles, eating habits, physical activity levels, and health indicators, such as: Age, Gender, Weight, BMI and Lable(Obesity Level)

##Importing Necessary Librarier
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn as dataset
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""##Loading the Dataset"""

df= pd.read_csv("Obesity Classification.csv")

"""##Analyze the dataset"""

df.head()

# @title Gender vs Label

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['Label'].value_counts()
    for x_label, grp in df.groupby('Gender')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('Gender')
_ = plt.ylabel('Label')

df.columns

df.shape

df.info()

"""##Checking for any null data"""

df.isnull().sum()

"""###There is no null/empty data in the dataset

##Checking for any Duplicate Data
"""

df.duplicated().sum()

"""###There is no duplicate data in the dataset"""

df.describe(include="all")

"""##Drop the 'ID' column, as it is not useful"""

df.drop(columns=['ID'], inplace=True)

df.head()

# @title Label vs Age

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(df['Label'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(df, x='Age', y='Label', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

"""###Decision tree algorithms typically require these categorical variables to be encoded into numerical representations before they can be used in the model.
Label encoding for the 'Gender' column and 'Label' column
"""

from sklearn.preprocessing import LabelEncoder

# Label encoding for the 'Gender' column
gender_encoder = LabelEncoder()
df['Gender_encoded'] = gender_encoder.fit_transform(df['Gender'])

# Label encoding for the 'Label' column
label_encoder = LabelEncoder()
df['Label_encoded'] = label_encoder.fit_transform(df['Label'])

df.head()

# Drop the 'Gender and label' column
df.drop(columns=['Gender','Label'], inplace=True)
df.head()

df.iloc[:,0:4]

x = df.iloc[:,0:6]
y = df.iloc[:,5]

x

y

"""##Splitting Data for traing the model"""

x=df.drop('Label_encoded',axis=1)
y=df.Label_encoded

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Initialize the decision tree classifier
dt_classifier = DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, criterion='gini')

# Fit the classifier to the training data
dt_classifier.fit(x_train, y_train)

# Predict on the testing data
y_pred = dt_classifier.predict(x_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Precision
precision = precision_score(y_test, y_pred, average='weighted')
print("Precision:", precision)

# Recall
recall = recall_score(y_test, y_pred, average='weighted')
print("Recall:", recall)

# F1-score
f1 = f1_score(y_test, y_pred, average='weighted')
print("F1-score:", f1)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""###Accuracy, Precision, Recall, and F1-score: The model achieved perfect scores of 1.0 for accuracy, precision, recall, and F1-score. This indicates that the model made no errors in classifying the test data. A perfect score suggests that the model performed exceptionally well and achieved optimal classification performance.
###Classification Report: The classification report provides a detailed breakdown of the model's performance for each class. For all classes (0, 1, 2, and 3), the precision, recall, and F1-score are 1.0, indicating perfect classification performance. The support column shows the number of samples for each class in the test set.
###Conclusion: The results indicate that the decision tree classifier performed flawlessly on the test data, achieving perfect classification accuracy for all classes. This suggests that the model was able to effectively learn and generalize the underlying patterns in the data. However, it's essential to consider the possibility of overfitting, especially when the model performs exceptionally well on the test data.
"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Convert class names to strings
class_names = dt_classifier.classes_.astype(str)

# Plot the decision tree
plt.figure(figsize=(20, 10))
plot_tree(dt_classifier, feature_names=x.columns, class_names=class_names, filled=True)
plt.show()

import seaborn as sns
from sklearn.metrics import confusion_matrix

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=dt_classifier.classes_, yticklabels=dt_classifier.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.model_selection import cross_val_score

# Perform cross-validation
scores = cross_val_score(dt_classifier, x, y, cv=5, scoring='accuracy')

# Print cross-validation scores
print("Cross-validation scores:", scores)

# Calculate and print mean cross-validation score
mean_score = scores.mean()
print("Mean cross-validation score:", mean_score)

"""###Mean Cross-Validation Score: The mean cross-validation score is approximately 0.954, indicating that, on average, the decision tree classifier achieved an accuracy of around 95.4% across the different folds. This suggests that the model performs well and consistently on unseen data.
###Cross-Validation Scores: The individual cross-validation scores range from 86.4% to 100%. While most of the folds achieved high accuracy (ranging from 90.5% to 100%), one fold had a slightly lower accuracy of 86.4%. Overall, the model demonstrates good generalization performance across different subsets of the data.
###Conclusion: The cross-validation results support the effectiveness and generalization ability of the decision tree classifier. The model's performance is consistent across multiple folds, indicating its reliability in classifying unseen data. However, it's essential to consider potential variations in performance across different subsets of the data
"""